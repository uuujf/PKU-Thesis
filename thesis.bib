@inproceedings{ahn2012bayesian,
  title={Bayesian posterior sampling via stochastic gradient fisher scoring},
  author={Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1771--1778},
  year={2012},
  organization={Omnipress}
}
@article{akiba2017extremely,
  title={Extremely large minibatch SGD: training resnet-50 on imagenet in 15 minutes},
  author={Akiba, Takuya and Suzuki, Shuji and Fukuda, Keisuke},
  journal={arXiv preprint arXiv:1711.04325},
  year={2017}
}
@article{ali2018continuous,
  title={A Continuous-Time View of Early Stopping for Least Squares Regression},
  author={Ali, Alnur and Kolter, J Zico and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1810.10082},
  year={2018}
}
@inproceedings{arpit2017closer,
  title={A Closer Look at Memorization in Deep Networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International Conference on Machine Learning},
  pages={233--242},
  year={2017}
}
@article{azizan2018stochastic,
  title={Stochastic gradient/mirror descent: Minimax optimality and implicit regularization},
  author={Azizan, Navid and Hassibi, Babak},
  journal={arXiv preprint arXiv:1806.00952},
  year={2018}
}
@inproceedings{bach2013non,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate O (1/n)},
  author={Bach, Francis and Moulines, Eric},
  booktitle={Advances in neural information processing systems},
  pages={773--781},
  year={2013}
}
@misc{blanc2019implicit,
    title={Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process},
    author={Guy Blanc and Neha Gupta and Gregory Valiant and Paul Valiant},
    year={2019},
    eprint={1904.09080},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{borkar1999strong,
  title={A strong approximation theorem for stochastic recursive algorithms},
  author={Borkar, Vivek S and Mitter, Sanjoy K},
  journal={Journal of optimization theory and applications},
  volume={100},
  number={3},
  pages={499--513},
  year={1999},
  publisher={Springer}
}
@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, L{\'e}on},
  journal={Proceedings of Neuro-N{\i}mes},
  volume={91},
  number={8},
  year={1991}
}
@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Springer}
}
@article{brutzkus2017sgd,
  title={SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1710.10174},
  year={2017}
}
@article{chaudhari2018deep,
  title={Deep relaxation: partial differential equations for optimizing deep neural networks},
  author={Chaudhari, Pratik and Oberman, Adam and Osher, Stanley and Soatto, Stefano and Carlier, Guillaume},
  journal={Research in the Mathematical Sciences},
  volume={5},
  number={3},
  pages={30},
  year={2018},
  publisher={Springer}
}
@article{chaudhari2017stochastic,
  title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author={Chaudhari, Pratik and Soatto, Stefano},
  journal={arXiv preprint arXiv:1710.11029},
  year={2017}
}
@inproceedings{chen2014stochastic,
  title={Stochastic gradient hamiltonian monte carlo},
  author={Chen, Tianqi and Fox, Emily and Guestrin, Carlos},
  booktitle={International Conference on Machine Learning},
  pages={1683--1691},
  year={2014}
}
@article{daneshmand2018escaping,
  title={Escaping Saddles with Stochastic Gradients},
  author={Daneshmand, Hadi and Kohler, Jonas and Lucchi, Aurelien and Hofmann, Thomas},
  journal={arXiv preprint arXiv:1803.05999},
  year={2018}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}
@inproceedings{duvenaud2016early,
  title={Early stopping as nonparametric variational inference},
  author={Duvenaud, David and Maclaurin, Dougal and Adams, Ryan},
  booktitle={Artificial Intelligence and Statistics},
  pages={1070--1077},
  year={2016}
}
@inproceedings{flammarion2015averaging,
  title={From averaging to acceleration, there is only a step-size},
  author={Flammarion, Nicolas and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={658--695},
  year={2015}
}
@techreport{friedman2003gradient,
  title={Gradient directed regularization for linear regression and classification},
  author={Friedman, J and Popescu, Bogdan E},
  year={2003},
  institution={Technical Report, Statistics Department, Stanford University}
}
@book{gardiner2009stochastic,
  title={Stochastic Methods: A Handbook for the Natural and Social Sciences},
  author={Gardiner, C.},
  isbn={9783540707127},
  lccn={2008936877},
  series={Springer Series in Synergetics},
  publisher={Springer Berlin Heidelberg},
  year={2018}
}
@article{goyal2017accurate,
  title={Accurate, large minibatch SGD: training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}
@article{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1802.08246},
  year={2018}
}
@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}
@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, M. and Recht, B. and Singer, Y.},
  booktitle={Proceedings of The 33rd International Conference on Machine Learning},
  pages={1225--1234},
  year={2016}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@incollection{hoffer2017,
title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {1731--1741},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, S. and Schmidhuber, J.},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}
@article{hu2017convergence,
  title={A convergence analysis of the perturbed compositional gradient flow: averaging principle and normal deviations},
  author={Hu, Wenqing and Li, Chris Junchi},
  journal={arXiv preprint arXiv:1709.00515},
  year={2017}
}
@article{hu2017diffusion,
  title={On the diffusion approximation of nonconvex stochastic gradient descent},
  author={Hu, Wenqing and Junchi Li, Chris and Li, Lei and Liu, Jian-Guo},
  journal={arXiv preprint arXiv:1705.07562},
  year={2017}
}
@article{hu2017fast,
  title={On the fast convergence of random perturbations of the gradient flow},
  author={Hu, Wenqing and Li, Chris Junchi},
  journal={arXiv preprint arXiv:1706.00837},
  year={2017}
}
@article{hu2017global,
  title={On the global convergence of a randomly perturbed dissipative nonlinear oscillator},
  author={Hu, Wenqing and Li, Chris Junchi and Su, Weijie},
  journal={arXiv preprint arXiv:1712.05733},
  year={2017}
}
@article{hu2019quasi,
  title={Quasi-potential as an implicit regularizer for the loss function in the stochastic gradient descent},
  author={Hu, Wenqing and Zhu, Zhanxing and Xiong, Haoyi and Huan, Jun},
  journal={arXiv preprint arXiv:1901.06054},
  year={2019}
}
@article{jastrzkebski2017three,
  title={Three Factors Influencing Minima in SGD},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}
@article{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}
@misc{jin2018local,
    title={On the Local Minima of the Empirical Risk},
    author={Chi Jin and Lydia T. Liu and Rong Ge and Michael I. Jordan},
    year={2018},
    eprint={1803.09357},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, N. S. and Mudigere, D. and Nocedal, J. and Smelyanskiy, M. and Tang, P. T. P.},
  booktitle={In International Conference on
Learning Representations (ICLR)},
  year={2017}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{kleinberg2018alternative,
  title={An Alternative View: When Does SGD Escape Local Minima?},
  author={Kleinberg, Robert and Li, Yuanzhi and Yuan, Yang},
  journal={arXiv preprint arXiv:1802.06175},
  year={2018}
}
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@article{kuzborskij2017data,
  title={Data-Dependent Stability of Stochastic Gradient Descent},
  author={Kuzborskij, Ilja and Lampert, Christoph},
  journal={arXiv preprint arXiv:1703.01678},
  year={2017}
}
@article{li2017algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  journal={arXiv preprint arXiv:1712.09203},
  year={2017}
}
@inproceedings{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={597--607},
  year={2017}
}
@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017}
}
@article{li2018stochastic,
  title={Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations},
  author={Li, Qianxiao and Tai, Cheng and others},
  journal={arXiv preprint arXiv:1811.01558},
  year={2018}
}
@article{mandt2017stochastic,
  title={Stochastic gradient descent as approximate Bayesian inference},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4873--4907},
  year={2017},
  publisher={JMLR. org}
}
@misc{martens2014new,
    title={New insights and perspectives on the natural gradient method},
    author={James Martens},
    year={2014},
    eprint={1412.1193},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015}
}
@article{meucci2009review,
  title={Review of statistical arbitrage, cointegration, and multivariate Ornstein-Uhlenbeck},
  author={Meucci, Attilio},
  year={2009}
}
@article{mou2017generalization,
  title={Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal={arXiv preprint arXiv:1707.05947},
  year={2017}
}
@article{nacson2018stochastic,
  title={Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate},
  author={Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
  journal={arXiv preprint arXiv:1806.01796},
  year={2018}
}
@article{neelakantan2015adding,
  title={Adding gradient noise improves learning for very deep networks},
  author={Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
  journal={arXiv preprint arXiv:1511.06807},
  year={2015}
}
@inproceedings{nesterov1983method,
  title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k\^{} 2)},
  author={Nesterov, Yurii},
  booktitle={Doklady AN USSR},
  volume={269},
  pages={543--547},
  year={1983}
}
@incollection{neyshabur2017,
title = {Exploring Generalization in Deep Learning},
author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5947--5956},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1705.03071},
  year={2017}
}
@article{neu2018iterate,
  title={Iterate averaging as regularization for stochastic gradient descent},
  author={Neu, Gergely and Rosasco, Lorenzo},
  journal={arXiv preprint arXiv:1802.08009},
  year={2018}
}
@incollection{oksendal2003stochastic,
    title={Stochastic differential equations},
    author={{\O}ksendal, Bernt},
    booktitle={Stochastic differential equations},
    pages={65--84},
    year={2003},
    publisher={Springer}
}
@book{pawitan2001all,
  title={In all likelihood: statistical modelling and inference using likelihood},
  author={Pawitan, Yudi},
  year={2001},
  publisher={Oxford University Press}
}
@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}
@article{raginsky2017non,
  title={Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1702.03849},
  year={2017}
}
@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}
@article{sagun2017empirical,
  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}
@inproceedings{shang2015covariance,
  title={Covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling},
  author={Shang, Xiaocheng and Zhu, Zhanxing and Leimkuhler, Benedict and Storkey, Amos J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={37--45},
  year={2015}
}
@article{shwartz2017opening,
  title={Opening the Black Box of Deep Neural Networks via Information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}
@article{simsekli2019tail,
  title={A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  journal={arXiv preprint arXiv:1901.06053},
  year={2019}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@article{smith2018bayesian,
title={A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
author={Samuel L. Smith and Quoc V. Le},
journal={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJij4yg0Z},
}
@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}
@inproceedings{suggala2018connecting,
  title={Connecting optimization and regularization paths},
  author={Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10608--10619},
  year={2018}
}
@inproceedings{Welling:2011:BLV:3104482.3104568,
 author = {Welling, Max and Teh, Yee Whye},
 title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
 booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
 series = {ICML'11},
 year = {2011},
 isbn = {978-1-4503-0619-5},
 location = {Bellevue, Washington, USA},
 pages = {681--688},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3104482.3104568},
 acmid = {3104568},
 publisher = {Omnipress},
 address = {USA},
}
@misc{wen2019interplay,
    title={Interplay Between Optimization and Generalization of Stochastic Gradient Descent with Covariance Noise},
    author={Yeming Wen and Kevin Luk and Maxime Gazeau and Guodong Zhang and Harris Chan and Jimmy Ba},
    year={2019},
    eprint={1902.08234},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@incollection{wilson2017marginal,
title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},
author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {4151--4161},
year = {2017},
}
@article{wu2017towards,
  title={Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}
@inproceedings{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and Weinan, E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8279--8288},
  year={2018}
}
@article{zeiler2012adadelta,
  title={ADADELTA: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@article{zhang2017hitting,
  title={A hitting time analysis of stochastic gradient langevin dynamics},
  author={Zhang, Yuchen and Liang, Percy and Charikar, Moses},
  journal={arXiv preprint arXiv:1702.05575},
  year={2017}
}
@inproceedings{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, C. and Bengio, S. and Hardt, M. and Recht, B. and Vinyals, O.},
  booktitle={International Conference on
Learning Representations},
  year={2017}
}
